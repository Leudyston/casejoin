1. Como você utiliza o Delta Lake no Azure Databricks para garantir a integridade dos dados?

É possível garantir a integridade dos dados por meio de transações ACID, controle de versionamento, enforcement de esquemas, operações idempotentes(reprocessar dados ou executar a mesma operação múltiplas vezes sem causar duplicações ou corrupção dos dados) e recursos de auditoria. Essas funcionalidades tornam o processo de gerenciamento de dados altamente confiável e seguro, mantendo a consistência e a integridade dos dados em escala.

2. Quais são as vantagens do uso do Spark em comparação com outras tecnologias de processamento de dados?
Spark já é conhecido por sua rapidez e flexibilidade. Ele faz um excelente trabalho ao processar dados de maneira eficiente, porque mantém os dados na memória, o que acelera bastante o trabalho. Além disso, o Spark pode lidar com diferentes tipos de dados e oferece uma interface única para várias tarefas, como análises de dados, aprendizado de máquina e processamento de dados em tempo real. Isso torna o Spark uma ferramenta poderosa para enfrentar grandes volumes de dados e realizar análises complexas de forma eficaz.

3. Descreva um caso em que você precisou sincronizar dados entre diferentes sistemas.

Em um grande cliente de Varejo ele tinha dados que vinnham do OMS e do Linx, os dados que vinham do OMS eram de vendas de apps e sites e os do Linx vinha das Lojas fisicas, por estarem com suas bases de dados em regioes com fusos horarios diferentes erá necessário sinconizar as cargas e o processamento dos dados com a datas em um unico fuso. Isso foi feito por meio de funções que foram criadas para alterar as colunas de datas do sistema com fuso horario diferente.

4. Desenhe uma arquitetura de dados comentada para uma empresa que utiliza Azure e Databricks, incluindo armazenamento, processamento e análise.

Coleta de Dados:
Data SOurces -> Azure Data Factory 
Use para movimentar e integrar esses dados de diferentes fontes para um local centralizado.

Armazenamento:
Dados Brutos e Dados Processados  -> Azure Blob Storage
Blob Storage é mais econômico e adequado para dados não estruturados.

Processamento:
Delta Live Tables (DLT):
Definir e executar pipelines ETL de forma simplificada e gerenciada. DLT fornece uma abordagem declarativa para a criação de pipelines, facilitando a ingestão e transformação de dados. Além de ajudas a garantir a qualidade dos dados com verificações e validações integradas durante o processo de transformação.

Análise de Dados:
Power BI - Analises com dashbords ou relatorios
Databricks Notebooks - Análises mais avançadas 
Databricks SQL Analytics - para analises internas de processmentos, armazenamento, custos...

Governança e Segurança:
Unity Catalog - gerenciar metadados e aplicar políticas de segurança e controle de acesso em tabelas Delta além de garantir a linhagems dos dados automaticamente.
Databricks Access Control
Databricks Audit Logs
Azure Active Directory (AAD)
Azure RBAC

CI/CD e Versionamento:
DevOps para Versionamento de Código - Versionar notebooks e pipelines
DevOps para Pipeline de CI/CD - Automatizar a integração e o deploy dos notebooks e pipelines do Databricks

5. Como você garante a escalabilidade e a robustez da arquitetura de dados?
Para garantir que sua arquitetura de dados seja escalável, você pode usar armazenamento e processamento distribuído para lidar com grandes volumes de dados. Técnicas como particionamento ajudam a organizar os dados de forma eficiente e otimizar as consultas para que tudo funcione de maneira rápida e fluida.

Quanto à robustez, é importante manter a qualidade dos dados usando transações ACID, que ajudam a garantir que tudo esteja sempre consistente e correto. Configure monitoramento e alertas para que você saiba imediatamente se algo não está funcionando como deveria. Tenha também uma estratégia sólida de backup e recuperação para não perder dados importantes em caso de problemas. Não se esqueça de implementar boas práticas de segurança e controle de acesso para proteger suas informações, e automatize testes e o processo de deploy para evitar erros e garantir que tudo esteja sempre atualizado e funcionando bem.

6. Como você implementa a criptografia de dados em repouso e em trânsito?
No Blob storage já tem criptografia nativa para dadso armazenados nesse caso os dados em repouso, para os dados em transito seria usar TLS/SSL pra criptografar as conexoes se for via API usar conexões seguras (HTTPS)

7. Como você gerencia a qualidade dos dados em um pipeline de dados?
Nesse caso da arqitetura proposta no DLT é possível criar alguns codigos a mais nos notebooks para automatizar o processo de validação e qualidade de dados.

8. Qual a importância do FinOps para a engenharia de dados?
Principalmente pra controle e previsão de custos, otimizar recursos e identificar se o investimento em dados ta alinhado com a estratégia e as metas da empresa.

9. Como o DevOps ajuda o engenheiro de dados?
Pipelines CI/CD, Versionamento de Codigo com a integração do GIT, colaboraçõa entre equipes, automação de processos repetitivos...

10. Como iniciamos um projeto de pipeline de dados?
Refinando Ação necessaria, a entrega de valor que essa ação vai gerar, os requisitos, os riscos.

11. Como realizar CI/CD em um pipeline de dados?
Criando e automatizando um processo que envolve automação de testes, integração e deployment de pipelines de dados. Configura inicialmente o repositorio de código, depois a integração conitnua (CI), em seguida a entrega continua, por fim monitora e gerencia versões e rollbacks. ë importante támbem manter a documentacao clara e atualziada.

12. Quais ferramentas de orquestração você já trabalhou?
Databricks, DataFactory e Synapse Analytics

13. Quais suas motivações para ser um engenheiro de dados?
Trabalho com Dados desde de 2006, porém somente em 2017 foquei na parte de inteligencia de dados, saber que o meu trabalho vai automatizar processos demorados, lentos e que podem ter custos elevados para empresas e instituiçoes, saber que essas informações geradas com todas essas transformações dos dados podem ajudar pessoas, empresas, a tomarem decisões mais assertivas e sem achismo, fora o fato de adorar tecnologia e ter prazer em desenvolver códigos cada vez mais performaticos. Enfim, tudo isso me motiva, já ouvi muito stack holders falando o quanto soluções criadas por mim ajudaram seu dia dia e diminuiram custos nas empresas, e isso me estimula a sempre fazer o meu melhor. 
